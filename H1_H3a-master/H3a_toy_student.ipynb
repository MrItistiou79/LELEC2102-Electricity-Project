{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hands-On 3a: Classification \n",
        "\n",
        "For this third hands-on session, let us go back to the signal processing aspects.\n",
        "Let us show again the block-diagram. H1 was focused on computing the feature vector from a sound vector. <br>\n",
        "Here, we are going to insert a simple classification model in our chain. This is the core of our application. <br>\n",
        "\n",
        "<center> <img src=\"images/block-diagram.png\" alt=\"\"  width=\"650\" height=\"350\"/> </center>\n",
        "\n",
        "There exists a lot of different classification models in the literature. In this notebook we propose to play with two simple and intuitive classifiers: the K-Nearest-Neighbour (``KNN``) classifier and the Support Vector Machine (``SVM``) classifier. The related Wikipedia page are [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and [SVM](https://en.wikipedia.org/wiki/Support-vector_machine). <br>\n",
        "Made simple, KNN and SVM are ``supervised machine learning algorithms``. For KNN, each new point to be classified is compared with its K nearest neighbours among labelled data (i.e. points whose class is known) in the dataspace with respect to a chosen distance metric. For SVM, a linear boundary is chosen for each pair of classes, eventually after a nonlinear transformation of the space to ease the linear separability of the data points.  <br>\n",
        "\n",
        "To avoid reinventing the wheel, we will use the implementations provided by ``sklearn``. You are strongly encouraged to have a look at the documentation [for the KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and [for the SVM](https://scikit-learn.org/stable/modules/svm.html). If not done yet, ``sklearn`` should be installed with pip. We will also use ``pickle`` for saving and loading the trained models. <br>\n",
        "In case of an error with the utils folder (folder not found), you may need to launch Jupyter with the directory where the code to execute is located. To do so, open the Anaconda Prompt (if you are using Anaconda) and type ``jupyter notebook --notebook-dir=$YOUR PATH$``. <br>\n",
        "Useful functions to select, read and play the dataset sounds are provided in the ``utils_`` and ``AudioUtil_And_Dataset`` folder. <br>\n",
        "\n",
        "As for the H1, you will have to fill some short pieces of code, as well as answer some questions. We already created cells for you to answer the questions to ensure you don't forget it ;). <br>\n",
        "You will find the zones to be briefly filled  with a ``### TO COMPLETE`` in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install sklearn\n",
        "# !pip install pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"Machine learning tools\"\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\"Self created functions\"\n",
        "from utils_ import get_accuracy, show_confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=6 color=#009999> 1. Getting intuition with a toy example </font> <br>\n",
        "\n",
        "Before going deep into classifying the complicated feature vectors we designed in H1, let us analyze a toy example. <br>\n",
        "It is always good practice to make some trials on a simplified version of a problem before tackling it. It prevents from a big amount of time loss and some strange dark bugs. <br>\n",
        "\n",
        "The convention used by ``sklearn`` is that $\\boldsymbol{X}$ is a matrix whose height is the number of classes and width is the dimension, as depicted here below (one colour is one class).\n",
        "\n",
        "<center> <img src=\"images/XY.svg\" alt=\"\"  width=\"400\" height=\"150\"/> </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot3D = False\n",
        "normalize = False\n",
        "N = 100 # Number of points per subcluster. (defaut: 100)\n",
        "n_classes = 4 # Number of differents classes. (default: 4)\n",
        "n_subclusters = 1 # Number of subclusters of points per class. (defaut: 1)\n",
        "dim = 2 # Dimensionality of the point clouds. (defaut: 2)\n",
        "dim_clusters = 2 # Dimensionality of the clusters arrangment. (default: 2)\n",
        "sig = 0.25 # Noise std. (default: 0.25)\n",
        "np.random.seed(9) # For some reproducibility in the results, you can change the seed or remove this line if desired. (default: 1)\n",
        "\n",
        "M = N * n_subclusters # Number of points per class\n",
        "\n",
        "\"Generate the data\"\n",
        "cluster_centers = np.concatenate((np.random.randn(n_classes*n_subclusters,dim_clusters), np.zeros((n_classes*n_subclusters, dim-dim_clusters))), axis=1)\n",
        "centers = np.repeat(cluster_centers, N, axis=0 )\n",
        "noise = sig*np.random.randn(n_classes*M,dim)\n",
        "X = centers + noise\n",
        "if (normalize):\n",
        "    X /= np.linalg.norm(X, axis=1, keepdims=True)\n",
        "y = np.repeat(np.arange(n_classes), M)\n",
        "\n",
        "print('Beware, the points are plotted in 2D but can belong to a space with more dimensions!')\n",
        "\n",
        "\"Plot\"\n",
        "cm = 'brg'\n",
        "edgc = 'k'\n",
        "fig = plt.figure()\n",
        "if (plot3D):\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.scatter3D(X[:,0],X[:,1],X[:,2], c=y, cmap=cm, edgecolor=edgc)\n",
        "else:\n",
        "    ax = plt.gca()\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "    scatterd = plt.scatter(X[:,0],X[:,1], c=y, cmap=cm, edgecolors=edgc)\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "handles, labels = scatterd.legend_elements(prop=\"colors\")\n",
        "ax.legend(handles, labels, title=\"Classes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 1.1. Metrics and model evaluation </font> <br>\n",
        "In order to objectively evaluate the performance of your classification model, the use of metrics is necessary.\n",
        "[See some examples here.](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226) <br>\n",
        "Throughout this notebook and for this project, the two metrics we will use are:\n",
        "* Accuracy = $\\frac{\\text{\\# Good predictions}}{\\text{Total \\# predictions}} = \\frac{\\text{TP+TN}}{\\text{TP+FP+FN+TN}}$.\n",
        "* Confusion matrix: add $1$ to the counter at position $(i,j)$ if the model predicted $i$ but the true label was $j$.\n",
        "\n",
        "If you are interested, there exists a lot of other metrics. Checkout on the internet. <br>\n",
        "\n",
        "We provide the code for the KNN classifier. For you to start handling sklearn implementations, we let you ``declare a SVM classifier and compute its accuracy on the test set``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"Shuffle the data then split in training and testing sets\"\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y) # 'stratify=y' ensures we pick the same number of samples per class during the splitting\n",
        "\n",
        "K=10 # Number of neighbours\n",
        "model_knn = KNeighborsClassifier(n_neighbors=K) # Declare the KNN classifier\n",
        "model_knn.fit(X_train, y_train) # Train the classifier\n",
        "prediction_knn = model_knn.predict(X_test)\n",
        "accuracy_knn = np.sum( prediction_knn-y_test==0)/X_test.shape[0]\n",
        "print('Accuracy KNN : {:.2f} %'.format(100*accuracy_knn))\n",
        "\n",
        "### TO COMPLETE\n",
        "model_svm = ... # Declare the SVM classifier with a linear kernel, C=1, and the other parameters by default.\n",
        "... # Train the classifier\n",
        "prediction_svm = ... # Predict the classes of the testing set\n",
        "accuracy_svm = np.sum( prediction_svm-y_test==0)/X_test.shape[0]\n",
        "print('Accuracy SVM : {:.2f} %'.format(100*accuracy_svm))\n",
        "\n",
        "\"Plot\"\n",
        "if (dim==2):\n",
        "    s=15.0\n",
        "    fig = plt.figure()\n",
        "    axs = [fig.add_axes([0.0, 0.0, 0.4, 0.9]), fig.add_axes([0.6, 0.0, 0.4, 0.9])]\n",
        "    # Plot the decision boundary. \n",
        "    n = 80\n",
        "    vec = np.linspace(np.min(X),np.max(X),n)\n",
        "    Xtmp = np.meshgrid(vec, vec)\n",
        "    Xtmp2 = np.array(Xtmp).reshape(2,n**2).T\n",
        "    axs[0].contourf(Xtmp[0], Xtmp[1], model_knn.predict(Xtmp2).reshape(n,n), cmap=cm, alpha=0.5)\n",
        "    axs[1].contourf(Xtmp[0], Xtmp[1], model_svm.predict(Xtmp2).reshape(n,n), cmap=cm, alpha=0.5)\n",
        "    axs[0].set_title('KNN')\n",
        "    axs[1].set_title('SVM')\n",
        "    for ax in axs:\n",
        "        scatterd = ax.scatter(X[:,0],X[:,1], c=y, cmap=cm, edgecolors=edgc, s=s)\n",
        "        handles, labels = scatterd.legend_elements(prop=\"colors\")\n",
        "        ax.set_aspect('equal', adjustable='box')\n",
        "        ax.set_xlabel('$x_1$')\n",
        "        ax.set_ylabel('$x_2$')\n",
        "        ax.legend(handles, labels, title=\"Classes\")\n",
        "    plt.show()\n",
        "\n",
        "plt.title('KNN')\n",
        "show_confusion_matrix (prediction_knn, y_test, np.arange(n_classes))\n",
        "plt.title('SVM using rbf kernel')\n",
        "show_confusion_matrix (prediction_svm, y_test, np.arange(n_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's already time for some analysis (put the default parameters when unspecified):\n",
        "\n",
        "1) Which accuracy would you expect from a classifier choosing uniformly at random?\n",
        "2) Can you explain why the classifiers are more confused between classes 0 and 2?\n",
        "3) Play with ``n_classes``, how do the accuracy and the confusion matrix evolve with it?\n",
        "4) Play with ``sig``, how do the accuracy and the confusion matrix evolve with it?\n",
        "5) Fix ``dim=20``, do you observe any change? An intuitive explanation?\n",
        "6) Fix ``dim=20`` and ``dim_clusters=20``, what do you observe?\n",
        "7) Change ``K`` to 200, what is the impact on the confusion matrix? Now choose ``n_subclusters``=2, what happens?\n",
        "8) How different are the decision boundaries between the KNN and the SVM with linear kernels? Which one would work best on the 3-ringes data example provided at the beginning of this notebook?\n",
        "9) Put ``normalize=True``. What do you observe in the data distribution? Think of the data points as if it corresponded to the acquired sounds. In which situation is it interesting to normalize? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE\n",
        "# Answer the questions above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 1.2. Dataset splitting and Model choice </font> <br>\n",
        "\n",
        "The usual convention to objectively analyse the performances of learned models is to split the dataset into three sets, ``learning, validation, testing`` where the validation set allows to choose the hyperparameters of each model. \n",
        "\n",
        "<center> <img src=\"images/dataset_splitting.svg\" alt=\"\"  width=\"600\" height=\"300\"/> </center>\n",
        "\n",
        "All the data in the learning and validation sets is used to train models and choose the hyperparameters that are optimal with respect to the chosen metrics, we call the ensemble the ``training set``. \n",
        "When training a model and comparing different settings, there is a risk that we will end up choosing optimal parameters that only renders good result on our specific case of training and validation set, but ``do not generalize well for additional data``. This is called ``overfitting on the validation set``. To alleviate this, we can perform ``cross-validation (CV)``. A basic approach named ``K-fold CV`` involves partitioning the dataset in ``K`` \"folds\" (subsets) and repetitvely do the following procedure:\n",
        "\n",
        "- Train the model using `K-1` folds as the training data.\n",
        "- Test the model using the last fold as the validation data.\n",
        "\n",
        "The overall performance on each fold is then averaged to obtain the final performance metrics.\n",
        "Alternatives to K-fold CV like [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) or other techniques exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean accuracy of KNN with 5-Fold CV: 76.8%\n",
            "Std deviation in accuracy of KNN with 5-Fold CV: 3.0% \n",
            "\n",
            "Mean accuracy of SVM with 5-Fold CV: 81.1%\n",
            "Std deviation in accuracy of SVM with 5-Fold CV: 2.7%\n"
          ]
        }
      ],
      "source": [
        "n_splits = 5\n",
        "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
        "\n",
        "accuracy_knn = np.zeros((n_splits,))\n",
        "accuracy_svm = np.zeros((n_splits,))\n",
        "for k, idx in enumerate(kf.split(X_train,y_train)):\n",
        "  (idx_learn, idx_val) = idx\n",
        "  model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "  prediction_knn = model_knn.predict(X_train[idx_val])\n",
        "  accuracy_knn[k] = get_accuracy(prediction_knn, y_train[idx_val])\n",
        "\n",
        "  model_svm.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "  prediction_svm = model_svm.predict(X_train[idx_val])\n",
        "  accuracy_svm[k] = get_accuracy(prediction_svm, y_train[idx_val])\n",
        "\n",
        "print('Mean accuracy of KNN with {}-Fold CV: {:.1f}%'.format(n_splits, 100*accuracy_knn.mean()))\n",
        "print('Std deviation in accuracy of KNN with 5-Fold CV: {:.1f}% \\n'.format(100*accuracy_knn.std()))\n",
        "print('Mean accuracy of SVM with {}-Fold CV: {:.1f}%'.format(n_splits, 100*accuracy_svm.mean()))\n",
        "print('Std deviation in accuracy of SVM with 5-Fold CV: {:.1f}%'.format(100*accuracy_svm.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the upper analysis, we fixed ``K`` for the KNN, and the ``kernel`` type and regularization parameter ``C`` for the SVM. These are called ``hyperparameters`` of the classification models. Let us now have a look at the effect of these hyperparameters!  <br>\n",
        "\n",
        "We give you the code for the KNN, and ask you to ``do essentially the same for the SVM``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_splits = 5\n",
        "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
        "\n",
        "Ks = np.arange(6,50, 2)\n",
        "accuracies_knn = np.zeros((len(Ks), n_splits))\n",
        "for i,K in enumerate(Ks):\n",
        "    model_knn = KNeighborsClassifier(n_neighbors=K) \n",
        "    for k, idx in enumerate(kf.split(X_train,y_train)):\n",
        "            (idx_learn, idx_val) = idx\n",
        "            model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
        "            prediction_knn = model_knn.predict(X_train[idx_val])\n",
        "            accuracies_knn[i,k] = get_accuracy(prediction_knn, y_train[idx_val])\n",
        "means_knn = accuracies_knn.mean(axis=1)\n",
        "stds_knn = accuracies_knn.std(axis=1)\n",
        "\n",
        "\n",
        "Cs = np.linspace(1e-1,1e1, 20)\n",
        "accuracies_svm = np.zeros((len(Cs), n_splits))\n",
        "\n",
        "### TO COMPLETE\n",
        "# Compute means_svm and stds_svm using the same procedure as above.\n",
        "\n",
        "means_svm = ...\n",
        "stds_svm = ...\n",
        "\n",
        "\"Plot\"\n",
        "fig = plt.figure(figsize=(12,3))\n",
        "axs = [fig.add_axes([0.0, 0.0, 0.45, 0.9]), fig.add_axes([0.55, 0.0, 0.45, 0.9])]\n",
        "axs[0].plot(Ks, means_knn, '.-b', label='KNN')\n",
        "axs[0].fill_between(Ks,means_knn-stds_knn,means_knn+stds_knn,alpha=0.2,color='b')\n",
        "axs[0].set_ylim(0,1)\n",
        "axs[0].set_xlabel('K')\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].legend()\n",
        "axs[1].plot(Cs, means_svm, '.-r', label='SVM')\n",
        "axs[1].fill_between(Cs,means_svm-stds_svm,means_svm+stds_svm,alpha=0.2,color='r')\n",
        "axs[1].set_ylim(0,1)\n",
        "axs[1].set_xlabel('C')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question:\n",
        "\n",
        "- Can you raise a dependency of the accuracy on K for the KNN and on C for the SVM here? Does it hold for other sets of parameters for the data generation? Comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE \n",
        "# Answer the questions above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we analysed the performance dependence on the hyperparameters, we can compare the two selected models on the ``test`` set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_K = Ks[np.argmax(means_knn)]\n",
        "best_C = Cs[np.argmax(means_svm)]\n",
        "\n",
        "print('Best K for KNN: {}'.format(best_K))\n",
        "print('Best C for SVM: {:.2f}'.format(best_C))\n",
        "\n",
        "model_best_knn = KNeighborsClassifier(n_neighbors=best_K)\n",
        "model_best_knn.fit(X_train, y_train) \n",
        "prediction_best_knn = model_best_knn.predict(X_test)\n",
        "accuracy_best_knn = get_accuracy(prediction_best_knn, y_test)\n",
        "print('Accuracy best KNN : {:.2f} %'.format(100*accuracy_best_knn))\n",
        "\n",
        "model_best_svm = SVC(kernel=\"linear\", C=best_C)\n",
        "model_best_svm.fit(X_train, y_train) \n",
        "prediction_best_svm = model_best_svm.predict(X_test)\n",
        "accuracy_best_svm = get_accuracy(prediction_best_svm, y_test)\n",
        "print('Accuracy best SVM : {:.2f} %'.format(100*accuracy_best_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question:\n",
        "- From the output here above, which model will you choose at the end?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TO COMPLETE \n",
        "# Answer the questions above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> Comment </font> <br>\n",
        "\n",
        "``You don't have to understand the code.`` <br>\n",
        "\n",
        "It can happen that the data to be classified is exploitable, but non linearly separable in their ambient space. It can be smart to find a transformation function $\\Phi (\\boldsymbol X)$\n",
        "that would ease the discrimination between your data points. For example, this is the trick used in [SVM](https://scikit-learn.org/stable/modules/svm.html) when radial basis functions or polynomial kernels are used (not shown here). Run the code here below and observe how well we are able to transform three intricate rings of data points into three localised point clouds using a technique called *spectral clustering*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 300 # Number of points per subcluster. (defaut: 100)\n",
        "n_classes = 3\n",
        "M = n_classes*N\n",
        "thetas = np.random.uniform(0,2*np.pi,M)\n",
        "\n",
        "X2 = ((np.array([np.cos(thetas),np.sin(thetas)]))*(np.arange(M)//N+0.1*np.random.randn(M))).T # The data points\n",
        "\n",
        "sigma = 0.1 \n",
        "W = np.zeros((M,M))\n",
        "for i in range(M):\n",
        "    for j in range(i):\n",
        "        dist = np.linalg.norm(X2[i,:]-X2[j,:])**2 # Should decrease with the distance!\n",
        "        W[i,j] = np.exp(-dist/(2*sigma**2))\n",
        "        W[j,i] = np.exp(-dist/(2*sigma**2))\n",
        "\n",
        "D = np.diag(np.sum(W,axis=0)) # Degree matrix for a weigthed graph\n",
        "L = D - W # Graph Laplacian\n",
        "(U,s,_) = np.linalg.svd(L, full_matrices=True) # SVD decomposition of L\n",
        "\n",
        "# Keep only K first eigenvectors\n",
        "K=3\n",
        "# Careful: SVD returns the singular values in DESCENDING order; we thus want to extract the LAST two columns\n",
        "U = U[:,-K:]\n",
        "\n",
        "fig = plt.figure(figsize=(7,3))\n",
        "axs = [fig.add_axes([0.55*i, 0, 0.45, 1]) for i in range(2)]\n",
        "for i in range(n_classes):\n",
        "        axs[0].scatter(X2[i*N:(i+1)*N,0],X2[i*N:(i+1)*N,1], label=i)\n",
        "        axs[1].scatter(U[i*N:(i+1)*N,0],U[i*N:(i+1)*N,1], s=5)\n",
        "axs[0].legend()\n",
        "axs[0].set_title('Data in initial space')\n",
        "axs[1].set_title('Data in transformed space')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size=5 color=#009999> 1.3. Dimensionality reduction </font> <br>\n",
        "\n",
        "It is sometimes good practice to reduce the dimensionality of a signal in order to get the main components of their distribution. A motivation is that usual norms behave counter-inuitively in high dimension. To reduce the dimensionality, we will use the ``Principal compenent analysis (PCA)`` proposed by sklearn. See the [associated Wikipedia page](https://en.wikipedia.org/wiki/Principal_component_analysis). We start by illustrating the interest of PCA with a toy example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 100 # Number of points per subcluster. (defaut: 100)\n",
        "n_classes = 4 # Number of differents classes. (default: 4)\n",
        "sig = 0.25 # Noise std. (default: 0.25)\n",
        "np.random.seed(8) # For some reproducibility in the results, you can change the seed or remove this line if desired. (default: 1)\n",
        "\n",
        "\"Generate the data\"\n",
        "xc = np.random.randn(n_classes)\n",
        "yc = 0.5*xc - 0.2\n",
        "cluster_centers = np.concatenate((xc[:,np.newaxis], yc[:,np.newaxis]), axis=1)\n",
        "centers = np.repeat(cluster_centers, N, axis=0 )\n",
        "noise = sig*np.random.randn(n_classes*N,2)\n",
        "X = centers + noise\n",
        "\n",
        "\"Apply PCA on data to reduce dimensionality to 1D\"\n",
        "n=1 # Number of principal components kept\n",
        "pca = PCA(n_components=n,whiten=True)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "\"Plot\"\n",
        "s=3.0\n",
        "fig = plt.figure()\n",
        "axs = [fig.add_axes([0.0,0.0,0.4,1.0]) , fig.add_axes([0.6,0.0,0.4,1.0])]\n",
        "axs[0].set_aspect('equal', adjustable='box')\n",
        "axs[1].set_aspect('equal', adjustable='box')\n",
        "for i in range(n_classes):\n",
        "    axs[0].scatter(X[i*N:(i+1)*N,0],X[i*N:(i+1)*N,1], label=i, s=s)\n",
        "    axs[1].scatter(X_reduced[i*N:(i+1)*N], np.zeros(N), label=i, s=s)\n",
        "axs[0].set_title('Original data living in 2D')\n",
        "axs[1].set_title('Data projected on the first principal component')\n",
        "axs[0].set_xlabel('$x_1$')\n",
        "axs[0].set_ylabel('$x_2$')\n",
        "axs[1].set_xlabel('$u_1$')\n",
        "axs[1].set_yticks([])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This hands-on session focuses on the simple KNN and SVM classifiers. However, there are many other that are worth giving a try using SKlearn. To give you motivation, run ``plot_classifier_comparison.py``"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "46df200377d403be22c796785365123e6a374b5da08e8292e6b2afda659c5a28"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}